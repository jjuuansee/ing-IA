{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 6: Resumen de la Unidad 1\n",
    "Este laboratorio tiene como objetivo resumir los conceptos aprendidos en la unidad 1 del curso.  A diferencia de los laboratorios anteriores, no se brinda código pre-armado, sino que se espera que los estudiantes implementen sus propias soluciones a los problemas planteados, tomando como referencia el trabajo en laboratorios anteriores.\n",
    "\n",
    "## Estructura del laboratorio\n",
    "1. Implementación de PCA sin utilizar librerarías externas.\n",
    "2. Exploración de usos de PCA sobre un conjunto de datos real (MNIST).\n",
    "3. Interpretación de las componentes principales.\n",
    "\n",
    "## Datos\n",
    "La celda que sigue carga el conjunto de datos MNIST que contiene imágenes de dígitos (del 0 al 9) escritos a mano.  Este dataset es ampliamente utilizado en la comunidad de aprendizaje automático y visión por computadora.  Cada imagen tiene un tamaño de 28x28 píxeles, lo que equivale a 784 características (una para cada píxel).  \n",
    "\n",
    "Este conjunto de datos fue generado por el Instituto Nacional de Estándares y Tecnología (NIST) de EE.UU. Originalmente, el conjunto de datos fue creado para la competencia de reconocimiento de dígitos escritos a mano, con el propósito de digitalizar formularios y documentos escritos a mano (cheques, formularios de impuestos, etc.) durante la década de 1980.  El conjunto de datos MNIST es una versión más pequeña y simplificada del conjunto de datos original NIST. Por más información, ver [MNIST en Wikipedia](https://es.wikipedia.org/wiki/Base_de_datos_MNIST).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de datos MNIST\n",
    "# NOTA: asegurese de tener instalado el paquete tensorflow\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# El dataset del MNIST, como todos los datasets para aprendizaje automático, se encuentra\n",
    "# dividido en dos partes: una parte de entrenamiento y otra de prueba. \n",
    "# Nosotros utilizaremos la parte de entrenamiento únicamente.\n",
    "X_mnist = X_train.reshape(X_train.shape[0], -1).T  # Aplanar las imágenes de entrenamiento\n",
    "y_mnist = y_train  # Etiquetas de las imágenes de entrenamiento\n",
    "\n",
    "#Nota: las imágenes de MNIST están representadas en uint8 (0-255).  \n",
    "# Donde 0 representa el negro, 255 el blanco y los valores intermedios son grises.\n",
    "# Para poder utilizar los datos algebraicamente, conviene convertirlos a punto flotante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema 1: Vectores propios asociados a los k valores propios más grandes de una matriz\n",
    "\n",
    "1. Escriba una función `eigenvectors(A, k)` que reciba una matriz cuadrada `A` y un entero `k` y devuelva los `k` vectores propios asociados a los `k` valores propios más grandes de `A`. Para ello, utilice el método de la potencia de forma recursiva. Primero halle el valor propio más grande de `A` y su vector propio asociado. Luego, calcule la matriz `A - \\lambda v.v^T` y aplique el método de la potencia a esta nueva matriz. Repita este proceso `k` veces. Puede asumir que no existirán valores propios repetidos (es decir, todos los valores propios son distintos).  \n",
    "2. Escriba una función `pca(X,k)` que reciba una matriz de datos `X` y un entero `k` y devuelva los `k` primeros componentes principales de `X`. Para ello, utilice la función `eigenvectors` de la parte anterior y operaciones matriciales básicas (sumas, restas, productos matriciales). Puede utilizarse como referencia el código del problema 3 del laboratorio 5, pero NO puede utilizarse la función `eig` o `eigh` de `numpy` u otras librerías.\n",
    "3. Aplique su método para hallar las 10 componentes principales del conjunto de datos MNIST. Compare sus resultados con los resultados de PCA de sklearn utilizando tests del tipo `assert` o `np.allclose`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parte 1\n",
    "def eigenvectors(A,k):\n",
    "    #COMPLETAR\n",
    "    return eigenvectors, eigenvalues\n",
    "\n",
    "\n",
    "# Parte 2\n",
    "def pca(X, k):\n",
    "    #COMPLETAR\n",
    "    return X_pca, principal_components, variance_explained\n",
    "\n",
    "\n",
    "#Parte 3: Aplicar a datos MNIST, comparar con sklearn\n",
    "#Función propia:\n",
    "X_pca, principal_components, variance_explained = pca(X_mnist, 10)\n",
    "\n",
    "#PCA de sklearn sobre los mismos datos (deben definirse las mismas variables):\n",
    "\n",
    "#Tests:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema 2: usos de PCA\n",
    "En el problema 4 del laboratorio 5 se utilizó PCA para comprimir datos del conjunto MNIST.  Esta es una posible aplicación de PCA (y de reducción de dimensionalidad en general) pero no es la única.  En este problema exploraremos esta aplicación en mayor profundidad y también el uso de reducción de dimensionalidad para visualizar conjuntos de datos de alta dimensión.  \n",
    "\n",
    "## Parte 1: Compresión de datos\n",
    "Cuando comprimimos un conjunto de datos mediante reducción de dimensionalidad, existe un residuo que no se puede representar en el espacio de menor dimensión.  Este residuo es la diferencia entre los datos originales y los datos proyectados en el espacio de menor dimensión.  El tamaño de este residuo puede medirse como la distancia euclidiana entre los datos originales y los datos proyectados, y en el caso de PCA, esto se relaciona directamente con la varianza no explicada por los componentes principales.\n",
    "1. Aplique PCA sobre los datos de MNIST.  Grafique la varianza explicada (como % de la varianza total) para todos los valores de `k` entre 0 y 784 (es decir, el número de características del conjunto de datos) vs. el valor de `k`.  \n",
    "2. Halle el valor de `k` necesario para explicar el 90%, 95%, y 99% de la varianza total.  \n",
    "3. Para los tres valores hallados en la parte anterior, visualice la reconstrucción de un conjunto de imágenes del dataset (utilice el mismo conjunto de imágenes con los tres valores de `k` para poder comparar).  \n",
    "\n",
    "## Parte 2: Visualización de datos\n",
    "Otra aplicación de PCA es la visualización de datos de alta dimensión.  Cuando tenemos un conjunto de datos como el del MNIST, que tiene 784 dimensiones (una para cada píxel de la imagen), es difícil visualizarlo directamente.  Sin embargo, podemos utilizar PCA para reducir la dimensionalidad del conjunto de datos a 2 o 3 dimensiones y graficar los datos en ese espacio reducido.  El subespacio dado por PCA es especialmente interesante, pues es el subespacio, para una dimensión dada, que mejor captura la dispersión de los datos en el espacio original.  \n",
    "1. Eligiendo 2 dimensiones al azar de las 784 del conjunto del MNIST, grafique los datos originales como un scatterplot.  Utilice un color distinto para cada dígito del 0 al 9 (clase). ¿Qué patrones o agrupaciones puede observar?  Intente con otras combinaciones de 2 dimensiones. ¿Se logran identificar patrones?\n",
    "2. Repita la gráfica anterior pero utilizando PCA para proyectar los datos en las 2 componentes principales del conjunto de datos. ¿Qué patrones o agrupaciones puede observar?  ¿Se logran identificar patrones?  ¿Qué diferencias observa entre la proyección de PCA y las gráficas de la parte anterior?\n",
    "3. Repita lo anterior para 3 dimensiones.  Utilice una visualización que le permita ver los datos en 3D y rotar la gráfica.  \n",
    "\n",
    " que es el .  Esto nos permite observar patrones o agrupaciones en los datos que podrían no ser evidentes en el espacio original de alta dimensión.\n",
    "\n",
    "En este caso, se busca proyectar los datos en un espacio de menor dimensión (2D o 3D) para poder graficarlos y observar patrones o agrupaciones.  En este caso, el objetivo no es necesariamente preservar la varianza, sino encontrar una representación que sea fácil de interpretar visualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETAR Problema 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problema 3: Interpretación de componentes principales\n",
    "Al usar PCA, proyectamos los datos en un espacio de menor dimensión.  Pero, ¿cuál es este subespacio? ¿Cuál es el sistema de coordenadas que estamos utilizando?  ¿Tiene algún significado?  Un acercamiento a estas preguntas es pensar en las componentes principales como direcciones en el espacio original de los datos.  En este sentido, cada componente principal puede verse como un vector, o elemento, de este espacio.  En el caso del MNIST, esto quiere decir que cada componente principal puede verse como una imagen de 28x28 píxeles.  En este problema, exploraremos esta interpretación de las componentes principales.\n",
    "\n",
    "1. Calcule las 5 componentes principales del conjunto de datos MNIST.  Para cada una, convierta el vector correspondiente a una imagen de 28x28 píxeles y visualícela utilizando `imshow` o una función similar.  ¿Qué patrones o agrupaciones puede observar?  ¿Qué información puede extraer de estas imágenes?\n",
    "**NOTA:** observe que las imágenes originales contenían valores entre 0 y 255, mientras que las componentes principales pueden tener valores negativos, entre -1 y 1 debido a la normalización de la base de vectores propios.  La correcta visualización de las imágenes requiere utilizar mapas de color apropiados (para ver valores positivos y negativos) o desplazar y normalizar los valores para asegurar que los mismos queden entre 0 y 255.  \n",
    "\n",
    "\n",
    "2. OPCIONAL: Repetir la parte anterior con dataset `olivetti_faces` disponible en `sklearn`.  Este dataset contiene imágenes de rostros humanos.  Para este dataset, las componentes principales pueden interpretarse como variaciones significativas en imágenes de caras.  ¿Qué patrones o agrupaciones puede observar?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Completar Problema 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
